{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 3\n",
    "embed_size = 2\n",
    "xmax = 2\n",
    "alpha = 0.75\n",
    "batch_size = 20\n",
    "l_rate = 0.001\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Golovkin won his first major world championship, the WBA interim middleweight title, by defeating Milton Núñez in 2010. The WBA elevated him to Regular champion status in the same year. In 2014, Golovkin was elevated to the status of WBA (Super) champion and successfully defended both his titles against Daniel Geale. Later that year he defeated Marco Antonio Rubio to win WBC interim middleweight title, and defeated David Lemieux for the IBF middleweight title in 2015. After Canelo Álvarez vacated his WBC middleweight title in 2016, Golovkin was elevated to full champion and held three of the four major world titles in boxing until being stripped by the IBF in 2018 for not fighting Serhiy Derevianchenko. Golovkin lost all his belts in 2018 following a loss to Alvarez but regained his IBF and IBO titles by defeating Derevianchenko in 2019.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus делится на tokens(слова) через точки и пробелы\n",
    "tokens = []\n",
    "for i in range(len(corpus)):\n",
    "    sents = corpus[i].split(\".\")\n",
    "    for j in range(len(sents)):\n",
    "        tokens = tokens +sents[j].lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#команда создает словарь\n",
    "my_dict = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#присвоение индекса к словам\n",
    "word2idx={}\n",
    "idx2word={}\n",
    "for ind,token in enumerate(my_dict):\n",
    "    word2idx[token]=ind\n",
    "    idx2word[ind]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создается нулевая таблица\n",
    "ss=len(my_dict)\n",
    "zero_matrix=np.zeros((ss,ss))\n",
    "#заполняется таблица совстречаемости \n",
    "for idx in range (len(tokens)-1):\n",
    "    ind1 = word2idx[tokens[idx]]\n",
    "    ind2 = word2idx[tokens[idx+1]]\n",
    "    zero_matrix[ind1,ind2]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#транспонирование таблицы\n",
    "coocs = np.transpose(np.nonzero(zero_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вычисление весов\n",
    "def wf(x):\n",
    "    if x < xmax:\n",
    "        return (x/xmax)**alpha\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вычисление левых и правых слов\n",
    "vocab_size=len(my_dict)\n",
    "w_list_size=len(tokens)\n",
    "left_emb, right_emb = [\n",
    "    [Variable(torch.from_numpy(np.random.normal(0, 0.01, (embed_size, 1))),\n",
    "        requires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
    "left_biases, right_biases = [\n",
    "    [Variable(torch.from_numpy(np.random.normal(0, 0.01, 1)), \n",
    "        requires_grad = True) for j in range(vocab_size)] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = optim.Adam(left_emb + right_emb + left_biases + right_biases, lr = l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch():\n",
    "    sample = np.random.choice(np.arange(len(coocs)), size=batch_size, replace=False)\n",
    "    left_vector, right_vector, cov, left_vector_biases, right_vector_biases = [], [], [], [], []\n",
    "    for chosen in sample:\n",
    "        ind = tuple(coocs[chosen])\n",
    "        left_vector.append(left_emb[ind[0]])\n",
    "        right_vector.append(right_emb[ind[1]])\n",
    "        cov.append(zero_matrix[ind])\n",
    "        left_vector_biases.append(left_biases[ind[0]])\n",
    "        right_vector_biases.append(right_biases[ind[1]])\n",
    "    return left_vector, right_vector, cov, left_vector_biases, right_vector_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тренировочная модель\n",
    "for epoch in range(num_epochs):\n",
    "    num_batches = int(w_list_size/batch_size)\n",
    "    average_loss = 0.0\n",
    "    for batch in range(num_batches):\n",
    "        optimize.zero_grad()\n",
    "        left_vector, right_vector, cov, left_vector_biases, right_vector_biases = gen_batch()\n",
    "        loss = sum([torch.mul((torch.dot(left_vector[i].view(-1), right_vector[i].view(-1)) +\n",
    "                left_vector_biases[i] + right_vector_biases[i] - np.log(cov[i]))**2,\n",
    "                wf(cov[i])) for i in range(batch_size)])\n",
    "        average_loss += loss.data[0]/num_batches\n",
    "        loss.backward()\n",
    "        optimize.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализация\n",
    "if embed_size == 2:\n",
    "    word_inds = np.random.choice(np.arange(len(my_dict)), size=10, replace=False)\n",
    "    for word_ind in word_inds:\n",
    "        w_embed = (left_emb[word_ind].data + right_emb[word_ind].data).numpy()\n",
    "        x, y = w_embed[0][0], w_embed[1][0]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(my_dict[word_ind], xy=(x, y), xytext=(5, 2),\n",
    "            textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.savefig(\"test_pic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
